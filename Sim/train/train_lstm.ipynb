{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Traffic Prediction LSTM (Anti-Mean Bias)\n",
    "This version implements advanced strategies to overcome the 'mean-prediction' problem and capture traffic spikes.\n",
    "\n",
    "### Architectual & Loss Fixes:\n",
    "1. **Weighted MSE Loss:** Penalizes errors on higher traffic counts more heavily, forcing the model to deviate from the mean.\n",
    "2. **Residual Bi-LSTM:** Adds skip connections between layers to preserve the original signal across the deep network.\n",
    "3. **Feature Scaling:** Uses `StandardScaler` to handle the relative variance of different directions.\n",
    "4. **Attention Head:** Refined Attention mechanism to isolate specific temporal triggers for traffic surges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering & Variance Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../lstm_training_data.csv')\n",
    "\n",
    "# 1. Cyclical Time Features\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# 2. Local Trends (Velocity of traffic change)\n",
    "for d in ['North', 'South', 'East', 'West']:\n",
    "    df[f'{d}_trend'] = df[d].diff().fillna(0)\n",
    "    df[f'{d}_roll_10'] = df[d].rolling(window=10).mean().fillna(method='bfill')\n",
    "\n",
    "feature_cols = [\n",
    "    'North', 'South', 'East', 'West', \n",
    "    'hour_sin', 'hour_cos', 'day_of_week',\n",
    "    'North_trend', 'South_trend', 'East_trend', 'West_trend',\n",
    "    'North_roll_10', 'South_roll_10', 'East_roll_10', 'West_roll_10'\n",
    "]\n",
    "target_cols = ['target_North', 'target_South', 'target_East', 'target_West']\n",
    "\n",
    "# Normalize features and targets separately\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "split_idx = int(len(df) * 0.8)\n",
    "X_train = scaler_x.fit_transform(df.iloc[:split_idx][feature_cols])\n",
    "Y_train = scaler_y.fit_transform(df.iloc[:split_idx][target_cols])\n",
    "X_test = scaler_x.transform(df.iloc[split_idx:][feature_cols])\n",
    "Y_test = scaler_y.transform(df.iloc[split_idx:][target_cols])\n",
    "\n",
    "joblib.dump(scaler_x, 'scaler_x.pkl')\n",
    "joblib.dump(scaler_y, 'scaler_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, x, y, lookback=45):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx : idx + self.lookback], self.y[idx + self.lookback]\n",
    "\n",
    "train_ds = TrafficDataset(X_train, Y_train)\n",
    "test_ds = TrafficDataset(X_test, Y_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model: Residual Attention-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, \n",
    "                            batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        \n",
    "        # Attention to weigh important timesteps\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply Attention\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        \n",
    "        return self.fc(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weighted Loss Function\n",
    "This is the most critical part for fixing the 'flat line' prediction. We penalize errors more if the actual value is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(preds, targets):\n",
    "    # Standard MSE\n",
    "    loss = (preds - targets) ** 2\n",
    "    \n",
    "    # Increase weight for points where target is > 0.5 standard deviations from mean\n",
    "    weights = torch.where(torch.abs(targets) > 0.5, 4.0, 1.0)\n",
    "    return (loss * weights).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResidualLSTM(len(feature_cols), 128, len(target_cols)).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "\n",
    "best_loss = float('inf')\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_train = 0\n",
    "    for bx, by in train_loader:\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(bx)\n",
    "        loss = weighted_mse_loss(pred, by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for bx, by in test_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            total_val += weighted_mse_loss(model(bx), by).item()\n",
    "    \n",
    "    val_loss = total_val/len(test_loader)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch} | Train Loss: {total_train/len(train_loader):.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification\n",
    "Checking if the model now tracks the spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "x_batch, y_batch = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    preds = model(x_batch.to(device)).cpu().numpy()\n",
    "\n",
    "preds_rescaled = scaler_y.inverse_transform(preds)\n",
    "actual_rescaled = scaler_y.inverse_transform(y_batch.numpy())\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(actual_rescaled[:200, 0], label='Actual (North)', alpha=0.8, color='blue')\n",
    "plt.plot(preds_rescaled[:200, 0], label='Predicted (North)', linestyle='--', color='red')\n",
    "plt.title('Improved Traffic Prediction with Spike Weighting')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": { "name": "ipython", "version": 3 },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
